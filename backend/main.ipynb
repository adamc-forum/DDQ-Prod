{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains steps for the DDQ Document Ingestion Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Union\n",
    "import re\n",
    "\n",
    "from document_processor_subclasses import (\n",
    "    MasterDDQProcessor,\n",
    "    OMProcessor,\n",
    "    ClientResponsesProcessor,\n",
    "    ClientResponseProcessor\n",
    ")\n",
    "\n",
    "from constants import (\n",
    "    TARGET_PDF_PATH,\n",
    "    CONNECTION_STRING,\n",
    "    DATABASE_NAME,\n",
    "    COLLECTION_NAME,\n",
    "    OPENAI_API_KEY,\n",
    "    OPENAI_API_VERSION,\n",
    "    OPENAI_ENDPOINT,\n",
    "    DI_ENDPOINT,\n",
    "    DI_API_KEY\n",
    ")\n",
    "\n",
    "from docx import Document\n",
    "from docx.document import Document as DocumentType\n",
    "from docx.table import Table\n",
    "from docx.text.paragraph import Paragraph\n",
    "\n",
    "from functions import(\n",
    "    get_openai_client,\n",
    "    get_db_client,\n",
    "    get_service_management_client,\n",
    "    get_models\n",
    ")\n",
    "\n",
    "from extractor import (\n",
    "    analyze_layout\n",
    ")\n",
    "\n",
    "from embeddings import (\n",
    "    generate_embeddings,\n",
    "    convert_chunks_to_json\n",
    ")\n",
    "\n",
    "from classes import (\n",
    "    DocumentChunk,\n",
    "    DocumentFlow\n",
    ")\n",
    "\n",
    "from document_parser import (\n",
    "    DocumentParser\n",
    ")\n",
    "\n",
    "from document_parser_utils import (\n",
    "    is_similar_color,\n",
    "    remove_non_alphanumeric\n",
    ")\n",
    "\n",
    "from azure.ai.formrecognizer import AnalyzeResult, DocumentParagraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = get_db_client()\n",
    "\n",
    "openai_client = get_openai_client()\n",
    "\n",
    "embedding_model, completions_model = get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database setup\n",
    "\n",
    "- Only needs to be run when setting up database collection and collection indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_client.setup_collection()\n",
    "\n",
    "db_client.create_indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Document Parsing\n",
    "\n",
    "- Makes API call to Azure DI for initial document parsing\n",
    "- Only needs to be run once per document, then analysis result in stored in pkl file for future access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_analysis_result = analyze_layout(TARGET_PDF_PATH, DI_ENDPOINT, DI_API_KEY)\n",
    "\n",
    "# print(document_analysis_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Chunking\n",
    "\n",
    "Modify this code as needed based on the document being parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in saved document analysis result from pkl file\n",
    "\n",
    "with open('layout_backup.pkl', 'rb') as file:\n",
    "    result = pickle.load(file)\n",
    "\n",
    "# Custom document parser class built on top of Azure DI output.\n",
    "# Required by document processor classes for chunking process\n",
    "    \n",
    "document_parser = DocumentParser(result=result)\n",
    "\n",
    "# Python-docx object\n",
    "# Required for determining headings based on styling\n",
    "\n",
    "document: DocumentType = Document(\n",
    "    TARGET_PDF_PATH.replace(\".pdf\", \".docx\")\n",
    ")\n",
    "\n",
    "filename = TARGET_PDF_PATH.split('/')[-1]\n",
    "\n",
    "client_response_processor = ClientResponseProcessor(document_parser, filename, document)\n",
    "\n",
    "# Custom document flow class that holds a list of chunks\n",
    "# Print document flow object to see processed chunks\n",
    "document_flow = client_response_processor.process_document()\n",
    "\n",
    "print(document_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploaing chunks to vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parsed chunks as json file\n",
    "with open(f\"{document_flow.client_name}_{document_flow.document_name}_parsing_backup.json\", \"w\") as file:\n",
    "    file.write(json.dumps(document_flow.to_dict()))\n",
    "\n",
    "# Vectorizes chunks and saves vectorized content to a backup json file\n",
    "vectorized_chunks = convert_chunks_to_json(document_flow.chunks, openai_client, embedding_model)\n",
    "\n",
    "# with open(f\"{document_flow.client_name}_{document_flow.document_name}_parsing_vectorized_backup.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     data = file.read()\n",
    "\n",
    "# Pushes vectorized chunks to db\n",
    "db_client.add_data_to_collection(vectorized_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
