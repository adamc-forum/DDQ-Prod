{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains steps for the DDQ Document Ingestion Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Union\n",
    "import re\n",
    "\n",
    "from document_processor_subclasses import (\n",
    "    MasterDDQProcessor,\n",
    "    OMProcessor,\n",
    "    ClientResponsesProcessor,\n",
    "    ClientResponseProcessor,\n",
    ")\n",
    "\n",
    "from constants import (\n",
    "    TARGET_PDF_PATH,\n",
    "    CONNECTION_STRING,\n",
    "    DATABASE_NAME,\n",
    "    COLLECTION_NAME,\n",
    "    OPENAI_API_KEY,\n",
    "    OPENAI_API_VERSION,\n",
    "    OPENAI_ENDPOINT,\n",
    "    DI_ENDPOINT,\n",
    "    DI_API_KEY,\n",
    "    SHAREPOINT_USR,\n",
    "    SHAREPOINT_PWD,\n",
    "    SHAREPOINT_URL,\n",
    "    SHAREPOINT_FOLDER,\n",
    ")\n",
    "\n",
    "from docx import Document\n",
    "from docx.document import Document as DocumentType\n",
    "from docx.table import Table\n",
    "from docx.text.paragraph import Paragraph\n",
    "\n",
    "from functions import (\n",
    "    get_openai_client,\n",
    "    get_db_client,\n",
    "    get_service_management_client,\n",
    "    get_models,\n",
    "    get_access_token,\n",
    "    get_sharepoint_headers,\n",
    "    get_sharepoint_site_id,\n",
    "    get_sharepoint_drive_id\n",
    ")\n",
    "\n",
    "from extractor import analyze_layout\n",
    "\n",
    "from embeddings import generate_embeddings, convert_chunks_to_json\n",
    "\n",
    "from classes import DocumentChunk, DocumentFlow\n",
    "\n",
    "from document_parser import DocumentParser\n",
    "\n",
    "from document_parser_utils import is_similar_color, remove_non_alphanumeric\n",
    "\n",
    "from azure.ai.formrecognizer import AnalyzeResult, DocumentParagraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = get_db_client()\n",
    "\n",
    "openai_client = get_openai_client()\n",
    "\n",
    "embedding_model, completions_model = get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database setup\n",
    "\n",
    "- Only needs to be run when setting up database collection and collection indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client.setup_collection()\n",
    "\n",
    "db_client.create_indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Document Parsing\n",
    "\n",
    "- Makes API call to Azure DI for initial document parsing\n",
    "- Only needs to be run once per document, then analysis result in stored in pkl file for future access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_analysis_result = analyze_layout(\n",
    "    TARGET_PDF_PATH, DI_ENDPOINT, DI_API_KEY)\n",
    "\n",
    "# print(document_analysis_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Chunking\n",
    "\n",
    "Modify this code as needed based on the document being parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in saved document analysis result from pkl file\n",
    "\n",
    "with open(\"layout_backup.pkl\", \"rb\") as file:\n",
    "    result = pickle.load(file)\n",
    "\n",
    "# Custom document parser class built on top of Azure DI output.\n",
    "# Required by document processor classes for chunking process\n",
    "\n",
    "document_parser = DocumentParser(result=result)\n",
    "\n",
    "# Python-docx object\n",
    "# Required for determining headings based on styling\n",
    "\n",
    "document: DocumentType = Document(TARGET_PDF_PATH.replace(\".pdf\", \".docx\"))\n",
    "\n",
    "filename = TARGET_PDF_PATH.split(\"/\")[-1]\n",
    "\n",
    "client_response_processor = ClientResponseProcessor(document_parser, filename, document)\n",
    "\n",
    "# Custom document flow class that holds a list of chunks\n",
    "# Print document flow object to see processed chunks\n",
    "document_flow = client_response_processor.process_document()\n",
    "\n",
    "print(document_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading chunks to vector db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parsed chunks as json file\n",
    "with open(\n",
    "    f\"{document_flow.client_name}_{document_flow.document_name}_parsing_backup.json\",\n",
    "    \"w\",\n",
    ") as file:\n",
    "    file.write(json.dumps(document_flow.to_dict()))\n",
    "\n",
    "# Vectorizes chunks and saves vectorized content to a backup json file\n",
    "vectorized_chunks = convert_chunks_to_json(\n",
    "    document_flow.chunks, openai_client, embedding_model\n",
    ")\n",
    "\n",
    "# with open(f\"{document_flow.client_name}_{document_flow.document_name}_parsing_vectorized_backup.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     data = file.read()\n",
    "\n",
    "# Pushes vectorized chunks to db\n",
    "db_client.add_data_to_collection(vectorized_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading pdf documents to sharepoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 https://forumequitypartners.sharepoint.com/sites/REIIFDDQAssistant/Shared%20Documents/General/GallantMacDonald_Responses_05-05-2023.pdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "access_token = get_access_token()\n",
    "\n",
    "headers = get_sharepoint_headers()\n",
    "\n",
    "graph_api_endpoint = \"https://graph.microsoft.com/v1.0/sites/forumequitypartners.sharepoint.com:/sites/REIIFDDQAssistant\"\n",
    "\n",
    "site_id = get_sharepoint_site_id(graph_api_endpoint)\n",
    "\n",
    "drive_id = get_sharepoint_drive_id(site_id)\n",
    "\n",
    "file_path = f\"General/{TARGET_PDF_PATH.split('/')[-1]}\"\n",
    "\n",
    "upload_api_endpoint = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{drive_id}/root:/{file_path}:/content\"\n",
    "\n",
    "with open(TARGET_PDF_PATH, \"rb\") as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "response = requests.put(upload_api_endpoint, headers=headers, data=file_content)\n",
    "uploaded_file_info = response.json()\n",
    "\n",
    "print(response.status_code, uploaded_file_info['webUrl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
